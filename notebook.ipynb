{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "270b328f",
   "metadata": {},
   "source": [
    "# Decision Support Layer — Notebook Overview\n",
    "\n",
    "**Goal:** The Decision Support Layer is designed to support event-driven, low-latency decision making based on continuous acoustic inputs. It interprets structured outputs from the Acoustic ML model using temporal aggregation and state-based logic. The resulting decisions are optimized for fast, human-facing alerts under real-time constraints..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a9c1d",
   "metadata": {},
   "source": [
    "## Architecture Diagrams\n",
    "\n",
    "### 1) Baseline: Acoustic Detection → Decision Support\n",
    "\n",
    "![Baseline architecture](docs/diagram-desicion-support-layer-baseline.png)\n",
    "\n",
    "### 2) Extension: Fusion-Enabled Decision Support\n",
    "\n",
    "![Fusion extension](docs/diagram-desicion-support-layer-fusion.png)\n",
    "\n",
    "> **Note:** These images are expected to be placed in the same repository folder as this notebook (or update the paths accordingly).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360dd11",
   "metadata": {},
   "source": [
    "## README\n",
    "\n",
    "# Event-Based Acoustic Decision Support Layer\n",
    "\n",
    "## Overview\n",
    "\n",
    "This repository implements a **decision-making layer** that operates on top of\n",
    "**event-based outputs produced by an Acoustic ML model**.\n",
    "\n",
    "The system **does not process raw audio signals** and does not perform signal processing.\n",
    "Instead, it consumes a **strict data contract** representing the output of an upstream\n",
    "Acoustic ML pipeline and transforms it into **time-aware, explainable decision outputs**\n",
    "suitable for user alerts.\n",
    "\n",
    "The project is centered around a single core principle:\n",
    "\n",
    "> **Acoustic ML output is the primary and mandatory input.  \n",
    "All other modalities are optional and must not block or delay decisions.**\n",
    "\n",
    "---\n",
    "\n",
    "## Core Concept\n",
    "\n",
    "### Primary responsibility\n",
    "\n",
    "- Define and enforce a **data contract** between:\n",
    "  - Acoustic ML inference\n",
    "  - Decision support logic\n",
    "- Aggregate acoustic detection events over time\n",
    "- Apply decision rules and state transitions\n",
    "- Produce structured outputs for alerting and visualization\n",
    "\n",
    "### Out of scope\n",
    "\n",
    "- Audio signal processing (DSP)\n",
    "- Acoustic feature extraction\n",
    "- ML model training or tuning\n",
    "- Hardware-level integration\n",
    "\n",
    "---\n",
    "\n",
    "## Input Data Contract (Mandatory)\n",
    "\n",
    "### Primary input: Acoustic ML events\n",
    "\n",
    "The **main input** to this system is the output of an Acoustic ML model that has already\n",
    "processed raw microphone data.\n",
    "\n",
    "This contract is **mandatory** and the system cannot operate without it.\n",
    "\n",
    "### File structure\n",
    "\n",
    "```\n",
    "input/\n",
    "  sessions/\n",
    "    session_<timestamp>__<id>/\n",
    "      acoustic_events.jsonl          ← REQUIRED\n",
    "      human_interaction.jsonl        ← OPTIONAL\n",
    "      video_observation.jsonl        ← OPTIONAL\n",
    "```\n",
    "\n",
    "Only `acoustic_events.jsonl` is required.  \n",
    "All other input files may be empty or absent.\n",
    "\n",
    "---\n",
    "\n",
    "### Acoustic Events Contract\n",
    "\n",
    "The acoustic event file represents **event-level ML inference results**, not signals.\n",
    "\n",
    "Typical fields include:\n",
    "- `timestamp`\n",
    "- `drone_detected: true | false`\n",
    "- `p_drone ∈ [0,1]`\n",
    "- `proximity: FAR | MID | CLOSE`\n",
    "- `eta_seconds` (optional)\n",
    "- `direction / distance` (optional)\n",
    "- `drone_presence_pattern: single | multiple`\n",
    "- `environment_context` (acoustically inferred noise conditions)\n",
    "\n",
    "Schema definition:\n",
    "```\n",
    "schemas/acoustic.events.schema.json\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Optional Fusion Inputs\n",
    "\n",
    "Optional inputs may provide context but must **never block decisions**.\n",
    "\n",
    "### Human Interaction (Optional)\n",
    "\n",
    "- Dictionary-based keyword events\n",
    "- No free speech recognition\n",
    "- No raw audio storage\n",
    "- Privacy-preserving by design\n",
    "\n",
    "Schema:\n",
    "```\n",
    "schemas/human_interaction.schema.json\n",
    "```\n",
    "\n",
    "### Video Observation (Optional)\n",
    "\n",
    "- Coarse observations only\n",
    "- `detected / not detected`\n",
    "- Low-frequency updates (~1 Hz)\n",
    "\n",
    "Schema:\n",
    "```\n",
    "schemas/video_observation.schema.json\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Support Layer\n",
    "\n",
    "Consumes acoustic (and optional fusion) events and performs:\n",
    "\n",
    "- Time-aware aggregation\n",
    "- State machine transitions\n",
    "- Alert suppression and cooldown handling\n",
    "- Confidence estimation\n",
    "\n",
    "### Decision States\n",
    "\n",
    "- **IDLE** — monitoring only\n",
    "- **ALERT** — alert issued to the user\n",
    "- **COOLDOWN** — alert already delivered, no repeated notifications\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Output Contract\n",
    "\n",
    "Decision results are always linked to input sessions.\n",
    "\n",
    "### Output structure\n",
    "\n",
    "```\n",
    "output/\n",
    "  sessions/\n",
    "    session_<timestamp>__<id>/\n",
    "      decision_output.jsonl\n",
    "```\n",
    "\n",
    "Schema:\n",
    "```\n",
    "schemas/decision_output.schema.json\n",
    "```\n",
    "\n",
    "Typical output fields:\n",
    "- `decision_state`\n",
    "- `p_drone`\n",
    "- `confidence_level`\n",
    "- `alert_triggered`\n",
    "- `alert_types`\n",
    "- `proximity`\n",
    "- `eta_seconds`\n",
    "- `explanation` (optional)\n",
    "\n",
    "---\n",
    "\n",
    "## Handling Multiple Objects\n",
    "\n",
    "Exact object counting is not performed.\n",
    "\n",
    "Instead:\n",
    "```\n",
    "drone_presence_pattern: single | multiple\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Design Principles\n",
    "\n",
    "- Acoustic-first design\n",
    "- Strict separation of concerns\n",
    "- Event-based processing\n",
    "- Time-aware decisions\n",
    "- Optional fusion inputs\n",
    "- Explainable outputs\n",
    "- Privacy-first human interaction\n",
    "\n",
    "---\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "This repository represents a **decision-layer prototype** for:\n",
    "- architecture validation\n",
    "- synthetic data replay\n",
    "- decision logic evaluation\n",
    "- visualization workflows\n",
    "\n",
    "---\n",
    "\n",
    "## Project Status\n",
    "\n",
    "- Data contracts: defined and stable\n",
    "- Synthetic sessions: included\n",
    "- Decision logic: prototype\n",
    "- Fusion extensions: future work\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6f66d",
   "metadata": {},
   "source": [
    "## Human Interaction Dictionary (Optional Channel)\n",
    "\n",
    "This channel is a **minimal, structured context** input. It is *not* raw audio, not free text, and does not label objects.\n",
    "\n",
    "**Fields:**\n",
    "- `space`: `inside | outside | open_area | forest | near_buildings`\n",
    "- `mobility`: `stationary | on_foot | in_vehicle`\n",
    "- `environment`: array of `wind | traffic_noise | quiet | urban_noise`\n",
    "- `visual_hint`: boolean (human has visual contact with an unidentified object)\n",
    "\n",
    "**Why:** reduce uncertainty / false positives while preserving privacy.\n",
    "\n",
    "Example event (JSONL line):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"timestamp_utc\": \"2026-01-26T21:05:00Z\",\n",
    "  \"source\": \"human_interaction\",\n",
    "  \"space\": \"outside\",\n",
    "  \"mobility\": \"on_foot\",\n",
    "  \"environment\": [\"quiet\"],\n",
    "  \"visual_hint\": true,\n",
    "  \"confidence\": 1.0,\n",
    "  \"privacy\": {\n",
    "    \"no_audio_content\": true,\n",
    "    \"no_image_data\": true,\n",
    "    \"no_object_labeling\": true\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
